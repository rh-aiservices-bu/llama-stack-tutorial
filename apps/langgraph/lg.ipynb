{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2pkcq63mtui",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 3: Use LangGraph's prebuilt ReAct agent with tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Create some real tools\n",
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "@tool \n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"Result: {result}\"\n",
    "    except:\n",
    "        return \"Invalid expression\"\n",
    "\n",
    "tools = [search_tool, calculator, get_weather]  # Use the weather tool from Option 1\n",
    "\n",
    "# Create ReAct agent with tools\n",
    "react_agent = create_react_agent(llm, tools=tools, debug=True)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"OPTION 3: LangGraph ReAct agent with real tools\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "response = react_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Search for the weather in Seattle and tell me what you find\")]\n",
    "})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"SUMMARY - Your options:\")\n",
    "print(\"=\"*30)\n",
    "print(\"1. ‚úÖ LangChain @tool decorator + bind_tools()\")\n",
    "print(\"2. ‚úÖ Native Llama Stack client for MCP\")  \n",
    "print(\"3. ‚úÖ LangGraph ReAct agent with tools\")\n",
    "print(\"4. ‚ùå MCP format with bind_tools() - NEVER works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ewm9b6g6l2q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Native Llama Stack MCP tools (bypass LangChain)\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Use Llama Stack client for real MCP support\n",
    "client = LlamaStackClient(\n",
    "    base_url=\"http://localhost:8321\",  # Fixed URL\n",
    "    timeout=600.0\n",
    ")\n",
    "\n",
    "def chatbot_with_mcp_tools(state: State):\n",
    "    # Convert messages\n",
    "    llama_messages = []\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, dict):\n",
    "            llama_messages.append(msg)\n",
    "        else:\n",
    "            role = \"user\" if hasattr(msg, 'type') and msg.type == \"human\" else \"user\"\n",
    "            if hasattr(msg, 'type') and msg.type == \"ai\":\n",
    "                role = \"assistant\"\n",
    "            llama_messages.append({\"role\": role, \"content\": msg.content})\n",
    "    \n",
    "    # Try MCP tools, fallback to basic\n",
    "    try:\n",
    "        response = client.inference.chat_completion(\n",
    "            model_id=INFERENCE_MODEL,\n",
    "            messages=llama_messages,\n",
    "            tools=[{\"type\": \"mcp\", \"tool_name\": \"weather\", \"server_label\": \"weather\"}]\n",
    "        )\n",
    "        print(\"‚úÖ Used MCP weather tool\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è MCP failed, using basic: {e}\")\n",
    "        response = client.inference.chat_completion(\n",
    "            model_id=INFERENCE_MODEL,\n",
    "            messages=llama_messages\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=response.completion_message.content)]}\n",
    "\n",
    "# Build graph with MCP tools\n",
    "graph_mcp = StateGraph(State)\n",
    "graph_mcp.add_node(\"chatbot\", chatbot_with_mcp_tools)\n",
    "graph_mcp.add_edge(START, \"chatbot\")\n",
    "graph_mcp.add_edge(\"chatbot\", END)\n",
    "graph_mcp_compiled = graph_mcp.compile()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"OPTION 2: Native Llama Stack MCP tools\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "response = graph_mcp_compiled.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n",
    "})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbt1mn2t4ss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Standard LangChain tools (WORKS with bind_tools)\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Create a proper LangChain weather tool\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get current weather for a location\"\"\"\n",
    "    # Mock weather data - replace with real API call\n",
    "    return f\"Weather in {location}: Sunny, 72¬∞F (22¬∞C), light breeze\"\n",
    "\n",
    "# This WORKS because it's proper LangChain format\n",
    "llm_with_langchain_tools = llm.bind_tools([get_weather])\n",
    "\n",
    "def chatbot_with_langchain_tools(state: State):\n",
    "    message = llm_with_langchain_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "# Build graph with LangChain tools\n",
    "graph_langchain = StateGraph(State)\n",
    "graph_langchain.add_node(\"chatbot\", chatbot_with_langchain_tools)\n",
    "graph_langchain.add_edge(START, \"chatbot\")\n",
    "graph_langchain.add_edge(\"chatbot\", END)\n",
    "graph_langchain_compiled = graph_langchain.compile()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"OPTION 1: LangChain tools (bind_tools compatible)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "response = graph_langchain_compiled.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n",
    "})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1tkjsiu2ew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED VERSION - Remove the problematic bind_tools line\n",
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Environment variables - FIXED URLs\n",
    "LLAMA_STACK_URL = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321\")  # Remove /v1/openai/v1\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\", \"ollama/llama3.2:3b-instruct-fp16\")\n",
    "INFERENCE_SERVER_OPENAI = os.getenv(\"LLAMA_STACK_ENDPOINT_OPENAI\", \"http://localhost:8321/v1/openai/v1\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", \"not-applicable\")\n",
    "\n",
    "print(\"LLAMA_STACK_URL: \", LLAMA_STACK_URL)\n",
    "print(\"INFERENCE_MODEL: \", INFERENCE_MODEL)\n",
    "\n",
    "# Working LLM setup\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    openai_api_key=API_KEY,  \n",
    "    openai_api_base=INFERENCE_SERVER_OPENAI,\n",
    "    use_responses_api=True,\n",
    ")\n",
    "\n",
    "# Test connectivity\n",
    "print(\"Testing basic connectivity:\")\n",
    "print(llm.invoke(\"Hello\"))\n",
    "\n",
    "# REMOVE THE PROBLEMATIC LINE - this will never work:\n",
    "# llm_with_tools = llm.bind_tools([{\"type\": \"mcp::weather\"}])\n",
    "\n",
    "# Instead, use the working LLM directly\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    # Use the working LLM without tools\n",
    "    message = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Test with your example\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing working LangGraph without MCP tools:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "response = graph.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital of Spain?\"}]\n",
    "})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*30)\n",
    "print(\"‚úÖ LangGraph works with basic LLM\")\n",
    "print(\"‚ùå MCP tools don't work with LangChain's bind_tools()\")\n",
    "print(\"üí° Use native Llama Stack client for MCP support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "klswc4h4mb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/sdgxccn96_56vbpwjjmt5yk40000gn/T/ipykernel_26514/2673790937.py:25: DeprecationWarning: /v1/inference/chat-completion is deprecated. Please use /v1/openai/v1/chat/completions.\n",
      "  basic_response = client.inference.chat_completion(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/v1/inference/chat-completion \"HTTP/1.1 404 Not Found\"\n",
      "/var/folders/n_/sdgxccn96_56vbpwjjmt5yk40000gn/T/ipykernel_26514/2673790937.py:57: DeprecationWarning: /v1/inference/chat-completion is deprecated. Please use /v1/openai/v1/chat/completions.\n",
      "  response = client.inference.chat_completion(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/v1/inference/chat-completion \"HTTP/1.1 404 Not Found\"\n",
      "/var/folders/n_/sdgxccn96_56vbpwjjmt5yk40000gn/T/ipykernel_26514/2673790937.py:71: DeprecationWarning: /v1/inference/chat-completion is deprecated. Please use /v1/openai/v1/chat/completions.\n",
      "  response = client.inference.chat_completion(\n",
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/v1/inference/chat-completion \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA_STACK_URL:  http://localhost:8321/v1/openai/v1\n",
      "INFERENCE_MODEL:  ollama/llama3.2:3b-instruct-fp16\n",
      "‚ùå Basic connectivity failed: Error code: 404 - {'detail': 'Not Found'}\n",
      "==================================================\n",
      "Testing MCP with fallback:\n",
      "==================================================\n",
      "‚ö†Ô∏è MCP failed (Error code: 404 - {'detail': 'Not Found'}), using basic inference...\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'detail': 'Not Found'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting MCP with fallback:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m response = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the weather in Seattle?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     95\u001b[39m     m.pretty_print()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mchatbot_with_fallback\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ö†Ô∏è MCP failed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), using basic inference...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINFERENCE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllama_messages\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=response.completion_message.content)]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/typing_extensions.py:3004\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   3001\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(arg)\n\u001b[32m   3002\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   3003\u001b[39m     warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3004\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/llama_stack_client/_utils/_utils.py:283\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/llama_stack_client/resources/inference.py:426\u001b[39m, in \u001b[36mInferenceResource.chat_completion\u001b[39m\u001b[34m(self, messages, model_id, logprobs, response_format, sampling_params, stream, tool_choice, tool_config, tool_prompt_format, tools, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    425\u001b[39m     extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext/event-stream\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/inference/chat-completion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msampling_params\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_config\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_prompt_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_prompt_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43minference_chat_completion_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInferenceChatCompletionParamsStreaming\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minference_chat_completion_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInferenceChatCompletionParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletionResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionResponseStreamChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AI_BU/llama-stack-tutorial/.venv-starter/lib/python3.12/site-packages/llama_stack_client/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'detail': 'Not Found'}",
      "During task with name 'chatbot' and id 'e5583b70-0317-3398-bea9-8655e2f720e6'"
     ]
    }
   ],
   "source": [
    "# Robust MCP Solution with fallback\n",
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Fixed environment variables\n",
    "LLAMA_STACK_URL = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321/v1/openai/v1\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\", \"ollama/llama3.2:3b-instruct-fp16\")\n",
    "\n",
    "print(\"LLAMA_STACK_URL: \", LLAMA_STACK_URL)\n",
    "print(\"INFERENCE_MODEL: \", INFERENCE_MODEL)\n",
    "\n",
    "# Native Llama Stack client\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_URL,\n",
    "    timeout=600.0\n",
    ")\n",
    "\n",
    "# Test basic connectivity first\n",
    "try:\n",
    "    basic_response = client.inference.chat_completion(\n",
    "        model_id=INFERENCE_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    )\n",
    "    print(\"‚úÖ Basic Llama Stack connectivity works\")\n",
    "    print(\"Response:\", basic_response.completion_message.content[:100] + \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Basic connectivity failed: {e}\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot_with_fallback(state: State):\n",
    "    # Convert LangGraph messages to Llama Stack format\n",
    "    llama_messages = []\n",
    "    for msg in state[\"messages\"]:\n",
    "        if isinstance(msg, dict):\n",
    "            llama_messages.append(msg)\n",
    "        else:\n",
    "            role = \"user\"\n",
    "            if hasattr(msg, 'type'):\n",
    "                if msg.type == \"human\":\n",
    "                    role = \"user\"\n",
    "                elif msg.type == \"ai\":\n",
    "                    role = \"assistant\"\n",
    "            llama_messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "    \n",
    "    # Try MCP first, fallback to basic if it fails\n",
    "    try:\n",
    "        response = client.inference.chat_completion(\n",
    "            model_id=INFERENCE_MODEL,\n",
    "            messages=llama_messages,\n",
    "            tools=[\n",
    "                {\n",
    "                    \"type\": \"mcp\",\n",
    "                    \"tool_name\": \"weather\",\n",
    "                    \"server_label\": \"weather\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        print(\"‚úÖ MCP weather tool worked!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è MCP failed ({e}), using basic inference...\")\n",
    "        response = client.inference.chat_completion(\n",
    "            model_id=INFERENCE_MODEL,\n",
    "            messages=llama_messages\n",
    "        )\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=response.completion_message.content)]}\n",
    "\n",
    "# Build StateGraph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot_with_fallback)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Testing MCP with fallback:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "response = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n",
    "})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb674a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langgraph langchain-openai langchain-core llama-stack-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b399f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49c97074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA_STACK_URL:  http://localhost:8321/v1/openai/v1\n",
      "INFERENCE_MODEL:  meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=[{'type': 'text', 'text': 'Hello! How can I assist you today?', 'annotations': []}] additional_kwargs={} response_metadata={'id': 'resp-31345b36-1c80-47f0-bad3-d185cdab72cf', 'created_at': 1757440659.0, 'model': 'meta-llama/Llama-3.2-3B-Instruct', 'object': 'response', 'status': 'completed', 'model_name': 'meta-llama/Llama-3.2-3B-Instruct'} id='msg_62ad7a94-96df-4f85-8b46-c48b7e56aeb0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:8321/v1/openai/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the weather in Seattle?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "[{'type': 'text', 'text': 'The current weather in Seattle is:\\n\\n* Temperature: 73¬∞F (23¬∞C)\\n* Wind: 6 mph NNW (north-northwest)\\n* Precipitation: Slight Chance Rain Showers\\n\\nPlease note that this is a fictional forecast and actual weather conditions may vary. For accurate forecast, please check with reliable sources such as National Weather Service or local news.', 'annotations': []}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Environment variables\n",
    "LLAMA_STACK_URL = os.getenv(\"LLAMA_STACK_URL\", \"http://localhost:8321/v1/openai/v1\")\n",
    "INFERENCE_MODEL = os.getenv(\"INFERENCE_MODEL\", \"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "INFERENCE_SERVER_OPENAI = os.getenv(\"LLAMA_STACK_ENDPOINT_OPENAI\", \"http://localhost:8321/v1/openai/v1\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\", \"not-applicable\")\n",
    "\n",
    "print(\"LLAMA_STACK_URL: \", LLAMA_STACK_URL)\n",
    "print(\"INFERENCE_MODEL: \", INFERENCE_MODEL)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=INFERENCE_MODEL,\n",
    "    openai_api_key=API_KEY,  \n",
    "    openai_api_base=INFERENCE_SERVER_OPENAI,\n",
    "    use_responses_api=True,\n",
    ")\n",
    "\n",
    "# Proof of connectivity\n",
    "print(llm.invoke(\"Hello\"))\n",
    "\n",
    "#llm_with_tools = llm.bind_tools([{\"type\": \"mcp::weather\"}])\n",
    "\n",
    "llm_with_tools = llm.bind_tools(\n",
    "    [          \n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"weather\",     \n",
    "            \"server_url\": \"http://host.containers.internal:3001/sse\",       \n",
    "            \"require_approval\": \"never\",\n",
    "        },\n",
    "    ])\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    #print(message)\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "response = graph.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in Seattle?\"}]})\n",
    "\n",
    "for m in response['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2648fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = True\n",
    "\n",
    "# This works because we're NOT using create_react_agent\n",
    "# Following the exact pattern from burrsutter's example\n",
    "llm_with_tools = llm.bind_tools([{\"type\": \"mcp::weather\"}])\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    message = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "# Build custom StateGraph (NOT create_react_agent)\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Test the agent\n",
    "response = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")]\n",
    "})\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-starter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
